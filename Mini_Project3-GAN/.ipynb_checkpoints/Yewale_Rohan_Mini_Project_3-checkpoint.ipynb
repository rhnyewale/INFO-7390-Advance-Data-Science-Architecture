{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLgxRd_DU-yX"
   },
   "source": [
    "# GANs (Generative Adversarial Networks)\n",
    "\n",
    "## Abstract\n",
    "\n",
    "Our objective is to learn how to implement Generative Adversarial Networks(GANs) using Keras and TensorFlow. We will be implementing on Fashion MNIST dataset.\n",
    "\n",
    "**Objective**\n",
    "1. Discuss Generative Adversarial Networks\n",
    "2. Implement GAN architecture using Keras and Tensorflow\n",
    "3. Train Fashion MNIST dataset\n",
    "4. Generate fake/synthetic apparel images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CceyJDFMVEyb"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "GANs are made up of two components -\n",
    "\n",
    "**Generator** - generates new data instances\n",
    "\n",
    "**Discriminator** - tries to distinguish the generated or fake data from the real dataset.\n",
    "\n",
    "Discriminative algorithms try to classify input data; that is, given the features of an instance of data, they predict a label or category to which that data belongs. So discriminative algorithms map features to labels. They are concerned solely with that correlation.One the other way,loosely speaking,generative algorithms do the opposite. Instead of predicting a label given certain features, they attempt to predict features given a certain label.\n",
    "\n",
    "While training they both start together from scratch and the generator learn to shape the random distribition through the training epochs.\n",
    "\n",
    "**Generative Adversarial Networks** can be used to generate synthetic (i.e., fake) images that are perceptually near identical to their ground-truth authentic originals.\n",
    "\n",
    "<Img src=\"https://github.com/rhnyewale/INFO-7390-Advance-Data-Science-Architecture/blob/master/Mini_Project3-GAN/Images/GANs.jpg?raw=true\">\n",
    "\n",
    "In order to generate synthetic images, we make use of two neural networks during training:\n",
    "\n",
    "1. A generator that accepts an input vector of randomly generated noise and produces an output “imitation” image that looks similar, if not identical, to the authentic image\n",
    "2. A discriminator or adversary that attempts to determine if a given image is an “authentic” or “fake”\n",
    "\n",
    "By training these networks at the same time, one giving feedback to the other, we can learn to generate synthetic images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dEi51ZhjVJWt"
   },
   "source": [
    "## General GAN training procedure\n",
    "\n",
    "<Img src=\"https://github.com/rhnyewale/INFO-7390-Advance-Data-Science-Architecture/blob/master/Mini_Project3-GAN/Images/GANs_Training%20.jpg?raw=true\">\n",
    "    \n",
    "    \n",
    "**Step 1** we randomly generate a vector (i.e., noise). We pass this noise through our generator,\n",
    "    \n",
    "**Step 2** which generates an actual image in **Step 2**\n",
    "    \n",
    "**Step 3** We then sample authentic images from our training set and mix them with our synthetic images\n",
    "\n",
    "**Step 4** next step is to train our discriminator using this mixed set. The goal of the discriminator is to correctly label each image as “real” or “fake.”\n",
    "\n",
    "**Step 5** Next, we’ll once again generate random noise, but this time we’ll purposely label each noise vector as a “real image” \n",
    "\n",
    "**Step 6** We’ll then train the GAN using the noise vectors and “real image” labels even though they are not actual real images\n",
    "    \n",
    "    \n",
    "**The reason this process works is due to the following:**\n",
    "\n",
    "1. We have frozen the weights of the discriminator at this stage, implying that the discriminator is not learning when we update the weights of the generator.\n",
    "    <br/><br/>\n",
    "2. We’re trying to “fool” the discriminator into being unable to determine which images are real vs. synthetic. The feedback from the discriminator will allow the generator to learn how to produce more authentic images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NcodFM0nM-o6"
   },
   "source": [
    "# Training the Network\n",
    "\n",
    "The important thing about training a GAN is that the two components should never be trained together.<br/> Rather the network is trained in two different phases\n",
    "\n",
    "1. The first phase is for training the discriminator and updating the weights appropriately\n",
    "2. In the next step the generator is trained while the discriminator training is disabled.\n",
    "\n",
    "**Phase 1** During phase one of training the generator is fed random data(in the form of a distribution) as noise. The generator creates some random images which are given to the discriminator. The discriminator also takes input from dataset of real images. The discriminator learns to distinguish the real data from the fake ones by learning or assessing features from it's inputs. The discriminator outputs some probability and difference between the predicted results and the actual results are backpropagated through the network and the weights of the discriminator is updated. Remember during this phase, the backpropagation stops at the end of the discriminator and the generator is not trained or updated.\n",
    "\n",
    "**Phase 2** In this phase, the generator produced batch of images are directly given as input to the discriminator. The real images are not given this time to the discriminator. The generator learns by tricking the discriminator into it outputting false positives. The discriminator outputs probabilities which are assessed against the actual results and the weights of of the generator are updated through backpropagation. Remember here during backpropagation, the discriminator weights should not be updated and kept as they were before.\n",
    "\n",
    "<Img src=\"https://github.com/rhnyewale/INFO-7390-Advance-Data-Science-Architecture/blob/master/Mini_Project3-GAN/Images/GANs_Training2.jpg?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "esF7PmHhoIgo"
   },
   "source": [
    "# Loss Function of GAN\n",
    "\n",
    "The **generator** tries to **minimize** the following function while<br/>\n",
    "the **discriminator** tries to **maximize** it:\n",
    "\n",
    "<Img src=\"https://github.com/rhnyewale/INFO-7390-Advance-Data-Science-Architecture/blob/master/Mini_Project3-GAN/Images/Loss_Function.jpg?raw=true\">\n",
    "\n",
    "\n",
    "In this function:\n",
    "\n",
    "* D(x) is the discriminator's estimate of the probability that real data instance x is real\n",
    "* Ex is the expected value over all real data instances\n",
    "* G(z) is the generator's output when given noise z\n",
    "* D(G(z)) is the discriminator's estimate of the probability that a fake instance is real\n",
    "* Ez is the expected value over all random inputs to the generator (in effect, the expected value over all generated fake instances G(z))\n",
    "\n",
    "\n",
    "The generator can't directly affect the log(D(x)) term in the function, so, for the generator, minimizing the loss is equivalent to minimizing log(1 - D(G(z)))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HQg4JQbydfH"
   },
   "source": [
    "# Application of GANs\n",
    "\n",
    "* Generate Examples for Image Datasets\n",
    "* Generate Photographs of Human Faces\n",
    "* Generate Realistic Photographs\n",
    "* Image-to-Image Translation\n",
    "* Text-to-Image Translation\n",
    "* Semantic-Image-to-Photo Translation\n",
    "* Photos to Emojis\n",
    "* Face Aging\n",
    "* Super Resolution\n",
    "* 3D Object Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QcycPog8VFqP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gV4LrXZpVCV1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zAckSQgGaGHY"
   },
   "source": [
    "### Install the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n9FkaOfWwawu",
    "outputId": "2d967de2-2417-44df-f05b-2ae569640d13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/be/679ce5254a8c8d07470efb4a4c00345fae91f766e64f1c2aece8796d7218/tensorflow-2.2.0-cp36-cp36m-manylinux2010_x86_64.whl (516.2MB)\n",
      "\u001b[K     |████████████████████████████████| 516.2MB 33kB/s \n",
      "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.2.0)\n",
      "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.3.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.15.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.10.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (3.3.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.33.2)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (2.10.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.6.3)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.1.2)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (0.35.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.18.5)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.1.0)\n",
      "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.4.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (1.12.1)\n",
      "Collecting tensorflow-estimator<2.3.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/f5/926ae53d6a226ec0fda5208e0e581cffed895ccc89e36ba76a8e60895b78/tensorflow_estimator-2.2.0-py2.py3-none-any.whl (454kB)\n",
      "\u001b[K     |████████████████████████████████| 460kB 46.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.2.0) (3.12.4)\n",
      "Collecting tensorboard<2.3.0,>=2.2.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1d/74/0a6fcb206dcc72a6da9a62dd81784bfdbff5fedb099982861dc2219014fb/tensorboard-2.2.2-py3-none-any.whl (3.0MB)\n",
      "\u001b[K     |████████████████████████████████| 3.0MB 49.4MB/s \n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorflow==2.2.0) (50.3.2)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.23.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.2)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.7.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.17.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.0.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.3.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2020.11.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.1.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (4.6)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (0.4.8)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.3.0,>=2.2.0->tensorflow==2.2.0) (3.4.0)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard, tensorflow\n",
      "  Found existing installation: tensorflow-estimator 2.3.0\n",
      "    Uninstalling tensorflow-estimator-2.3.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
      "  Found existing installation: tensorboard 2.3.0\n",
      "    Uninstalling tensorboard-2.3.0:\n",
      "      Successfully uninstalled tensorboard-2.3.0\n",
      "  Found existing installation: tensorflow 2.3.0\n",
      "    Uninstalling tensorflow-2.3.0:\n",
      "      Successfully uninstalled tensorflow-2.3.0\n",
      "Successfully installed tensorboard-2.2.2 tensorflow-2.2.0 tensorflow-estimator-2.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcrOk6pURp50"
   },
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VJaCNlDDRz6d"
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "from matplotlib import pyplot as plt\n",
    "from imutils import build_montages\n",
    "from sklearn.utils import shuffle\n",
    "import numpy as np\n",
    "import argparse\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lBrLwCtN5kqy"
   },
   "source": [
    "### Function to display images in Jupyter Notebooks and Google Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fRw969Dp5Kdm"
   },
   "outputs": [],
   "source": [
    "def plt_imshow(title, image):\n",
    "  # convert the image frame BGR to RGB color space and display it\n",
    "\timage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\tplt.imshow(image, aspect=\"auto\")\n",
    "\tplt.title(title)\n",
    "\tplt.grid(False)\n",
    "\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W54NJvphSiac"
   },
   "source": [
    "**Transposed convolutional layers**, sometimes referred to as **fractionally-strided convolution** or (incorrectly) **deconvolution**, are used when we need a transform going in the opposite direction of a normal convolution.\n",
    "\n",
    "The generator of our GAN will accept an N dimensional input vector (i.e., a list of numbers, but a volume like an image) and then transform the N dimensional vector into an output image.\n",
    "\n",
    "This process implies that we need to reshape and then upscale this vector into a volume as it passes through the network — to accomplish this reshaping and upscaling, we’ll need transposed convolution.\n",
    "\n",
    "**Transposed convolution** as the method to:\n",
    "\n",
    "1. Accept an input volume from a previous layer in the network\n",
    "2. Produce an output volume that is larger than the input volume\n",
    "3. Maintain a connectivity pattern between the input and output\n",
    "\n",
    "In essence our transposed convolution layer will reconstruct our target spatial resolution and perform a normal convolution operation, utilizing fancy zero-padding techniques to ensure our output spatial dimensions are met."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jppw5-Bd56H-"
   },
   "source": [
    "### Implementing our “generator” and “discriminator” with Keras and TensorFlow\n",
    "\n",
    "Here we define the build_generator function inside DCGAN.\n",
    "The build_generator accepts a number of arguments.\n",
    "\n",
    "* dim: The target spatial dimensions (width and height) of the generator after reshaping\n",
    "* depth: The target depth of the volume after reshaping\n",
    "* channels: The number of channels in the output volume from the generator (i.e., 1 for grayscale images and 3 for RGB images)\n",
    "* inputDim: Dimensionality of the randomly generated input vector to the generator\n",
    "* outputDim: Dimensionality of the output fully-connected layer from the randomly generated input vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cwHgIBergNRm"
   },
   "outputs": [],
   "source": [
    "class DCGAN:\n",
    "\t@staticmethod\n",
    "\tdef build_generator(dim, depth, channels=1, inputDim=100,\n",
    "\t\toutputDim=512):\n",
    "\t\t# initialize the model along with the input shape to be\n",
    "\t\t# \"channels last\" and the channels dimension itself\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinputShape = (dim, dim, depth)\n",
    "\t\tchanDim = -1\n",
    "\n",
    "        # first set of FC => RELU => BN layers\n",
    "\t\tmodel.add(Dense(input_dim=inputDim, units=outputDim))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "\n",
    "\t\t# second set of FC => RELU => BN layers, this time preparing\n",
    "\t\t# the number of FC nodes to be reshaped into a volume\n",
    "\t\tmodel.add(Dense(dim * dim * depth))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization())\n",
    "  \n",
    "  \t\t# reshape the output of the previous layer set, upsample +\n",
    "\t\t# apply a transposed convolution, RELU, and BN\n",
    "\t\tmodel.add(Reshape(inputShape))\n",
    "\t\tmodel.add(Conv2DTranspose(32, (5, 5), strides=(2, 2),\n",
    "\t\t\tpadding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"relu\"))\n",
    "\t\tmodel.add(BatchNormalization(axis=chanDim))\n",
    "  \n",
    "  \t\t# apply another upsample and transposed convolution, but\n",
    "\t\t# this time output the TANH activation\n",
    "\t\tmodel.add(Conv2DTranspose(channels, (5, 5), strides=(2, 2),\n",
    "\t\t\tpadding=\"same\"))\n",
    "\t\tmodel.add(Activation(\"tanh\"))\n",
    "\n",
    "\t\t# return the generator model\n",
    "\t\treturn model\n",
    "\n",
    "# While the generator is intended to create synthetic images,\n",
    "# the discriminator is used to classify whether any given input image is real or fake.\n",
    "\n",
    "\t@staticmethod\n",
    "\tdef build_discriminator(width, height, depth, alpha=0.2):\n",
    "\t\t# initialize the model along with the input shape to be\n",
    "\t\t# \"channels last\"\n",
    "\t\tmodel = Sequential()\n",
    "\t\tinputShape = (height, width, depth)\n",
    "\n",
    "\t\t# first set of CONV => RELU layers\n",
    "\t\tmodel.add(Conv2D(32, (5, 5), padding=\"same\", strides=(2, 2),\n",
    "\t\t\tinput_shape=inputShape))\n",
    "\t\tmodel.add(LeakyReLU(alpha=alpha))\n",
    "\n",
    "\t\t# second set of CONV => RELU layers\n",
    "\t\tmodel.add(Conv2D(64, (5, 5), padding=\"same\", strides=(2, 2)))\n",
    "\t\tmodel.add(LeakyReLU(alpha=alpha))\n",
    "\n",
    "\t\t# first (and only) set of FC => RELU layers\n",
    "\t\tmodel.add(Flatten())\n",
    "\t\tmodel.add(Dense(512))\n",
    "\t\tmodel.add(LeakyReLU(alpha=alpha))\n",
    "\n",
    "\t\t# sigmoid layer outputting a single value\n",
    "\t\tmodel.add(Dense(1))\n",
    "\t\tmodel.add(Activation(\"sigmoid\"))\n",
    "  \n",
    "    # All activation layers utilize a Leaky ReLU activation to stabilize training,\n",
    "    # except for the final activation function which is sigmoid.\n",
    "    # We use a sigmoid here to capture the probability of whether the input image is real or synthetic.\n",
    "\n",
    "\t\t# return the discriminator model\n",
    "\t\treturn model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3p5IY4Dfsa8"
   },
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oDMMjdvg13X"
   },
   "source": [
    "### Implementing our GAN training script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "okM7Bpyeq8Kc"
   },
   "outputs": [],
   "source": [
    "# parsing code with *hard coded* arguments and values\n",
    "args = {\n",
    "\t\"output\": \"output\",\n",
    "\t\"epochs\": 50,\n",
    "\t\"batch_size\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Zgwh3CUjhe6j"
   },
   "outputs": [],
   "source": [
    "# store the epochs and batch size in convenience variables, then\n",
    "# initialize our learning rate\n",
    "NUM_EPOCHS = args[\"epochs\"]\n",
    "BATCH_SIZE = args[\"batch_size\"]\n",
    "INIT_LR = 2e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Npe914Lvh04x"
   },
   "source": [
    "## Loading Fashion MNIST dataset\n",
    "Class labels can be ignored here, since we do not need them — we are only interested in the actual pixel data.\n",
    "\n",
    "Furthermore, there is no concept of a “test set” for GANs. Our goal when training a GAN isn’t minimal loss or high accuracy. Instead, we seek an equilibrium between the generator and the discriminator.\n",
    "\n",
    "To help us obtain this equilibrium, we combine both the training and testing images to give us additional training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LfCBguOqhjKq",
    "outputId": "6709ed6d-3009-4537-8a8f-2c3917ba77e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading MNIST dataset...\n"
     ]
    }
   ],
   "source": [
    "# load the Fashion MNIST dataset and stack the training and testing\n",
    "# data points so we have additional training data\n",
    "print(\"[INFO] loading MNIST dataset...\")\n",
    "((trainX, _), (testX, _)) = fashion_mnist.load_data()\n",
    "trainImages = np.concatenate([trainX, testX]) #To equilibrium, combine both the training and testing images to give additional training data.\n",
    "\n",
    "# add in an extra dimension for the channel and scale the images\n",
    "# into the range [-1, 1] (which is the range of the tanh\n",
    "# function)\n",
    "trainImages = np.expand_dims(trainImages, axis=-1)\n",
    "trainImages = (trainImages.astype(\"float\") - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S0NasxSYdMFV",
    "outputId": "c04fc6b4-92fb-4b6c-e0b7-a03b12bb4f63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building generator...\n"
     ]
    }
   ],
   "source": [
    "# build the generator\n",
    "print(\"[INFO] building generator...\")\n",
    "gen = DCGAN.build_generator(7, 64, channels=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yxngSuf5dO7P",
    "outputId": "1db1ae3c-1583-4e50-9491-4add2c681d34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               51712     \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3136)              1608768   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 3136)              0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 3136)              12544     \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 32)        51232     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 14, 14, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 14, 14, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 1)         801       \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 28, 28, 1)         0         \n",
      "=================================================================\n",
      "Total params: 1,727,233\n",
      "Trainable params: 1,719,873\n",
      "Non-trainable params: 7,360\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbQUwppsdWyB"
   },
   "source": [
    "First, our model will accept an input vector that is 100-d, then transform it to a 512-d vector via an FC layer.\n",
    "\n",
    "We then add a second FC layer, this one with 7x7x64 = 3,136 nodes. We reshape these 3,136 nodes into a 3D volume with shape 7×7 = 64 — this reshaping is only possible since our previous FC layer matches the number of nodes in the reshaped volume.\n",
    "\n",
    "Applying a transposed convolution with a 2×2 stride increases our spatial dimensions from 7×7 to 14×14.\n",
    "\n",
    "A second transposed convolution (again, with a stride of 2×2) increases our spatial dimension resolution from 14×14 to 28×18 with a single channel, which is the exact dimensions of our input images in the Fashion MNIST dataset.\n",
    "\n",
    "When implementing your own GANs, make sure the spatial dimensions of the output volume match the spatial dimensions of your input images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wi4dCP3khkdo",
    "outputId": "b3ff0da8-7af9-4a1b-c6a1-519ff114d534"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building discriminator...\n"
     ]
    }
   ],
   "source": [
    "# build the discriminator\n",
    "print(\"[INFO] building discriminator...\")\n",
    "disc = DCGAN.build_discriminator(28, 28, 1)\n",
    "discOpt = Adam(lr=INIT_LR, beta_1=0.5, decay=INIT_LR / NUM_EPOCHS)\n",
    "disc.compile(loss=\"binary_crossentropy\", optimizer=discOpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsi5O7XnopJ3"
   },
   "source": [
    "We are using **binary cross-entropy** here, as our **discriminator** has a **sigmoid activation** function that will return a probability indicating whether the input image is real vs. fake. Since there are only two “class labels” (real vs. synthetic), we use binary cross-entropy.\n",
    "\n",
    "The learning rate and beta value for the Adam optimizer were experimentally tuned. Lower learning rate and beta value for the Adam optimizer improves GAN training on the Fashion MNIST dataset. Applying learning rate decay helps stabilize training as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BC-vCwEhhnKE",
    "outputId": "02375a40-fada-482e-bb64-1f6aaa485cff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] building GAN...\n"
     ]
    }
   ],
   "source": [
    "# build the adversarial model by first setting the discriminator to\n",
    "# *not* be trainable, then combine the generator and discriminator\n",
    "# together\n",
    "print(\"[INFO] building GAN...\")\n",
    "disc.trainable = False  # first we need to freeze the discriminator weights before we combine the models to form our GAN\n",
    "ganInput = Input(shape=(100,))\n",
    "ganOutput = disc(gen(ganInput))\n",
    "gan = Model(ganInput, ganOutput)\n",
    "\n",
    "# compile the GAN\n",
    "ganOpt = Adam(lr=INIT_LR, beta_1=0.5, decay=INIT_LR / NUM_EPOCHS)\n",
    "gan.compile(loss=\"binary_crossentropy\", optimizer=discOpt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5x_Ac5GtzTE"
   },
   "source": [
    "The input to the gan will take a random vector that is 100-d. This value will be passed through the generator first, the output of which will go to the discriminator.\n",
    "\n",
    "The discriminator weights are frozen at this point so the feedback from the discriminator will enable the generator to learn how to generate better synthetic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SPes5dYwhqL5",
    "outputId": "c87c9308-9384-4a81-b2bb-3911a102d0d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] starting training...\n",
      "[INFO] starting epoch 1 of 50...\n",
      "[INFO] Step 1_0: discriminator_loss=0.697817, adversarial_loss=0.607017\n",
      "[INFO] Step 1_25: discriminator_loss=0.076382, adversarial_loss=0.020839\n",
      "[INFO] Step 1_50: discriminator_loss=0.000513, adversarial_loss=0.001891\n",
      "[INFO] Step 1_75: discriminator_loss=0.000072, adversarial_loss=0.000716\n",
      "[INFO] Step 1_100: discriminator_loss=0.000192, adversarial_loss=0.000370\n",
      "[INFO] Step 1_125: discriminator_loss=0.000037, adversarial_loss=0.000240\n",
      "[INFO] Step 1_150: discriminator_loss=0.000041, adversarial_loss=0.000199\n",
      "[INFO] Step 1_175: discriminator_loss=0.000057, adversarial_loss=0.000213\n",
      "[INFO] Step 1_200: discriminator_loss=0.000104, adversarial_loss=0.000238\n",
      "[INFO] Step 1_225: discriminator_loss=0.000113, adversarial_loss=0.001642\n",
      "[INFO] Step 1_250: discriminator_loss=0.201734, adversarial_loss=3.204310\n",
      "[INFO] Step 1_275: discriminator_loss=0.369591, adversarial_loss=1.197738\n",
      "[INFO] Step 1_300: discriminator_loss=0.476401, adversarial_loss=2.171065\n",
      "[INFO] Step 1_325: discriminator_loss=0.372979, adversarial_loss=1.484729\n",
      "[INFO] Step 1_350: discriminator_loss=0.460049, adversarial_loss=0.814836\n",
      "[INFO] Step 1_375: discriminator_loss=0.428965, adversarial_loss=1.030028\n",
      "[INFO] Step 1_400: discriminator_loss=0.411368, adversarial_loss=1.136538\n",
      "[INFO] Step 1_425: discriminator_loss=0.370165, adversarial_loss=1.448529\n",
      "[INFO] Step 1_450: discriminator_loss=0.612944, adversarial_loss=1.773850\n",
      "[INFO] Step 1_475: discriminator_loss=0.431600, adversarial_loss=1.168317\n",
      "[INFO] Step 1_500: discriminator_loss=0.447696, adversarial_loss=0.893422\n",
      "[INFO] Step 1_525: discriminator_loss=0.567625, adversarial_loss=0.416924\n",
      "[INFO] Step 1_545: discriminator_loss=0.455180, adversarial_loss=1.197769\n",
      "[INFO] starting epoch 2 of 50...\n",
      "[INFO] Step 2_0: discriminator_loss=0.455705, adversarial_loss=1.284901\n",
      "[INFO] Step 2_25: discriminator_loss=0.468046, adversarial_loss=1.054297\n",
      "[INFO] Step 2_50: discriminator_loss=0.479611, adversarial_loss=1.083835\n",
      "[INFO] Step 2_75: discriminator_loss=0.576171, adversarial_loss=1.353752\n",
      "[INFO] Step 2_100: discriminator_loss=0.502335, adversarial_loss=1.171612\n",
      "[INFO] Step 2_125: discriminator_loss=0.519136, adversarial_loss=1.511088\n",
      "[INFO] Step 2_150: discriminator_loss=0.502463, adversarial_loss=1.564851\n",
      "[INFO] Step 2_175: discriminator_loss=0.483860, adversarial_loss=1.390696\n",
      "[INFO] Step 2_200: discriminator_loss=0.474247, adversarial_loss=1.089950\n",
      "[INFO] Step 2_225: discriminator_loss=0.495955, adversarial_loss=0.902066\n",
      "[INFO] Step 2_250: discriminator_loss=0.503953, adversarial_loss=0.982377\n",
      "[INFO] Step 2_275: discriminator_loss=0.577115, adversarial_loss=0.644727\n",
      "[INFO] Step 2_300: discriminator_loss=0.514813, adversarial_loss=1.123103\n",
      "[INFO] Step 2_325: discriminator_loss=0.548862, adversarial_loss=0.727041\n",
      "[INFO] Step 2_350: discriminator_loss=0.510272, adversarial_loss=1.080170\n",
      "[INFO] Step 2_375: discriminator_loss=0.467080, adversarial_loss=1.372659\n",
      "[INFO] Step 2_400: discriminator_loss=0.523632, adversarial_loss=1.346260\n",
      "[INFO] Step 2_425: discriminator_loss=0.504455, adversarial_loss=1.301723\n",
      "[INFO] Step 2_450: discriminator_loss=0.476566, adversarial_loss=1.064521\n",
      "[INFO] Step 2_475: discriminator_loss=0.511916, adversarial_loss=1.430212\n",
      "[INFO] Step 2_500: discriminator_loss=0.544044, adversarial_loss=1.253916\n",
      "[INFO] Step 2_525: discriminator_loss=0.592403, adversarial_loss=0.572090\n",
      "[INFO] Step 2_545: discriminator_loss=0.527698, adversarial_loss=0.912078\n",
      "[INFO] starting epoch 3 of 50...\n",
      "[INFO] Step 3_0: discriminator_loss=0.554764, adversarial_loss=1.225868\n",
      "[INFO] Step 3_25: discriminator_loss=0.624858, adversarial_loss=0.725624\n",
      "[INFO] Step 3_50: discriminator_loss=0.537663, adversarial_loss=1.173839\n",
      "[INFO] Step 3_75: discriminator_loss=0.593318, adversarial_loss=1.613446\n",
      "[INFO] Step 3_100: discriminator_loss=0.532922, adversarial_loss=0.850528\n",
      "[INFO] Step 3_125: discriminator_loss=0.637157, adversarial_loss=0.723074\n",
      "[INFO] Step 3_150: discriminator_loss=0.558991, adversarial_loss=1.030934\n",
      "[INFO] Step 3_175: discriminator_loss=0.558408, adversarial_loss=0.572767\n",
      "[INFO] Step 3_200: discriminator_loss=0.536648, adversarial_loss=1.214295\n",
      "[INFO] Step 3_225: discriminator_loss=0.553631, adversarial_loss=0.745742\n",
      "[INFO] Step 3_250: discriminator_loss=0.534252, adversarial_loss=0.896793\n",
      "[INFO] Step 3_275: discriminator_loss=0.534290, adversarial_loss=0.987245\n",
      "[INFO] Step 3_300: discriminator_loss=0.589373, adversarial_loss=1.515637\n",
      "[INFO] Step 3_325: discriminator_loss=0.574557, adversarial_loss=1.183909\n",
      "[INFO] Step 3_350: discriminator_loss=0.550958, adversarial_loss=1.004506\n",
      "[INFO] Step 3_375: discriminator_loss=0.541944, adversarial_loss=1.043045\n",
      "[INFO] Step 3_400: discriminator_loss=0.593581, adversarial_loss=0.783955\n",
      "[INFO] Step 3_425: discriminator_loss=0.635683, adversarial_loss=1.510171\n",
      "[INFO] Step 3_450: discriminator_loss=0.588999, adversarial_loss=0.788763\n",
      "[INFO] Step 3_475: discriminator_loss=0.567532, adversarial_loss=0.814956\n",
      "[INFO] Step 3_500: discriminator_loss=0.582660, adversarial_loss=1.224461\n",
      "[INFO] Step 3_525: discriminator_loss=0.580445, adversarial_loss=1.170643\n",
      "[INFO] Step 3_545: discriminator_loss=0.560504, adversarial_loss=1.008696\n",
      "[INFO] starting epoch 4 of 50...\n",
      "[INFO] Step 4_0: discriminator_loss=0.575804, adversarial_loss=0.822788\n",
      "[INFO] Step 4_25: discriminator_loss=0.560841, adversarial_loss=1.140380\n",
      "[INFO] Step 4_50: discriminator_loss=0.623794, adversarial_loss=1.224823\n",
      "[INFO] Step 4_75: discriminator_loss=0.560966, adversarial_loss=0.658592\n",
      "[INFO] Step 4_100: discriminator_loss=0.581533, adversarial_loss=0.875736\n",
      "[INFO] Step 4_125: discriminator_loss=0.589638, adversarial_loss=1.348834\n",
      "[INFO] Step 4_150: discriminator_loss=0.602559, adversarial_loss=0.684165\n",
      "[INFO] Step 4_175: discriminator_loss=0.608715, adversarial_loss=0.910883\n",
      "[INFO] Step 4_200: discriminator_loss=0.543503, adversarial_loss=0.882346\n",
      "[INFO] Step 4_225: discriminator_loss=0.579513, adversarial_loss=1.095785\n",
      "[INFO] Step 4_250: discriminator_loss=0.577122, adversarial_loss=0.774956\n",
      "[INFO] Step 4_275: discriminator_loss=0.632876, adversarial_loss=0.598265\n",
      "[INFO] Step 4_300: discriminator_loss=0.564156, adversarial_loss=1.015909\n",
      "[INFO] Step 4_325: discriminator_loss=0.595446, adversarial_loss=0.780733\n",
      "[INFO] Step 4_350: discriminator_loss=0.577207, adversarial_loss=0.996675\n",
      "[INFO] Step 4_375: discriminator_loss=0.553858, adversarial_loss=1.043387\n",
      "[INFO] Step 4_400: discriminator_loss=0.663098, adversarial_loss=0.568735\n",
      "[INFO] Step 4_425: discriminator_loss=0.597014, adversarial_loss=0.776756\n",
      "[INFO] Step 4_450: discriminator_loss=0.595450, adversarial_loss=0.690414\n",
      "[INFO] Step 4_475: discriminator_loss=0.582873, adversarial_loss=1.036708\n",
      "[INFO] Step 4_500: discriminator_loss=0.642368, adversarial_loss=1.153242\n",
      "[INFO] Step 4_525: discriminator_loss=0.600644, adversarial_loss=1.046039\n",
      "[INFO] Step 4_545: discriminator_loss=0.606863, adversarial_loss=0.866925\n",
      "[INFO] starting epoch 5 of 50...\n",
      "[INFO] Step 5_0: discriminator_loss=0.602963, adversarial_loss=0.929180\n",
      "[INFO] Step 5_25: discriminator_loss=0.607818, adversarial_loss=0.878073\n",
      "[INFO] Step 5_50: discriminator_loss=0.611076, adversarial_loss=1.087665\n",
      "[INFO] Step 5_75: discriminator_loss=0.590822, adversarial_loss=1.250968\n",
      "[INFO] Step 5_100: discriminator_loss=0.604367, adversarial_loss=1.389738\n",
      "[INFO] Step 5_125: discriminator_loss=0.608749, adversarial_loss=0.847139\n",
      "[INFO] Step 5_150: discriminator_loss=0.614321, adversarial_loss=0.803786\n",
      "[INFO] Step 5_175: discriminator_loss=0.612521, adversarial_loss=1.063986\n",
      "[INFO] Step 5_200: discriminator_loss=0.582312, adversarial_loss=0.933954\n",
      "[INFO] Step 5_225: discriminator_loss=0.616072, adversarial_loss=1.198870\n",
      "[INFO] Step 5_250: discriminator_loss=0.581499, adversarial_loss=1.000557\n",
      "[INFO] Step 5_275: discriminator_loss=0.674826, adversarial_loss=0.621927\n",
      "[INFO] Step 5_300: discriminator_loss=0.604328, adversarial_loss=1.035197\n",
      "[INFO] Step 5_325: discriminator_loss=0.611803, adversarial_loss=0.806052\n",
      "[INFO] Step 5_350: discriminator_loss=0.571881, adversarial_loss=0.819120\n",
      "[INFO] Step 5_375: discriminator_loss=0.593022, adversarial_loss=1.331786\n",
      "[INFO] Step 5_400: discriminator_loss=0.597559, adversarial_loss=0.844998\n",
      "[INFO] Step 5_425: discriminator_loss=0.591217, adversarial_loss=0.882364\n",
      "[INFO] Step 5_450: discriminator_loss=0.613878, adversarial_loss=0.705470\n",
      "[INFO] Step 5_475: discriminator_loss=0.600697, adversarial_loss=0.726943\n",
      "[INFO] Step 5_500: discriminator_loss=0.631199, adversarial_loss=1.066580\n",
      "[INFO] Step 5_525: discriminator_loss=0.608407, adversarial_loss=0.793631\n",
      "[INFO] Step 5_545: discriminator_loss=0.616563, adversarial_loss=0.745509\n",
      "[INFO] starting epoch 6 of 50...\n",
      "[INFO] Step 6_0: discriminator_loss=0.618029, adversarial_loss=0.930224\n",
      "[INFO] Step 6_25: discriminator_loss=0.591648, adversarial_loss=0.854921\n",
      "[INFO] Step 6_50: discriminator_loss=0.605442, adversarial_loss=1.126755\n",
      "[INFO] Step 6_75: discriminator_loss=0.592249, adversarial_loss=1.024882\n",
      "[INFO] Step 6_100: discriminator_loss=0.610681, adversarial_loss=0.814305\n",
      "[INFO] Step 6_125: discriminator_loss=0.601006, adversarial_loss=0.847429\n",
      "[INFO] Step 6_150: discriminator_loss=0.624672, adversarial_loss=0.966848\n",
      "[INFO] Step 6_175: discriminator_loss=0.758526, adversarial_loss=0.632428\n",
      "[INFO] Step 6_200: discriminator_loss=0.591059, adversarial_loss=1.157624\n",
      "[INFO] Step 6_225: discriminator_loss=0.606838, adversarial_loss=0.746600\n",
      "[INFO] Step 6_250: discriminator_loss=0.614440, adversarial_loss=0.676763\n",
      "[INFO] Step 6_275: discriminator_loss=0.663609, adversarial_loss=0.555947\n",
      "[INFO] Step 6_300: discriminator_loss=0.580357, adversarial_loss=0.793206\n",
      "[INFO] Step 6_325: discriminator_loss=0.647882, adversarial_loss=0.661947\n",
      "[INFO] Step 6_350: discriminator_loss=0.608151, adversarial_loss=1.241841\n",
      "[INFO] Step 6_375: discriminator_loss=0.599877, adversarial_loss=1.236789\n",
      "[INFO] Step 6_400: discriminator_loss=0.610094, adversarial_loss=0.658492\n",
      "[INFO] Step 6_425: discriminator_loss=0.598161, adversarial_loss=0.847277\n",
      "[INFO] Step 6_450: discriminator_loss=0.595888, adversarial_loss=0.668823\n",
      "[INFO] Step 6_475: discriminator_loss=0.606482, adversarial_loss=0.749986\n",
      "[INFO] Step 6_500: discriminator_loss=0.574492, adversarial_loss=0.937154\n",
      "[INFO] Step 6_525: discriminator_loss=0.609486, adversarial_loss=1.055375\n",
      "[INFO] Step 6_545: discriminator_loss=0.599260, adversarial_loss=0.847441\n",
      "[INFO] starting epoch 7 of 50...\n",
      "[INFO] Step 7_0: discriminator_loss=0.595052, adversarial_loss=0.995092\n",
      "[INFO] Step 7_25: discriminator_loss=0.591968, adversarial_loss=0.812028\n",
      "[INFO] Step 7_50: discriminator_loss=0.620806, adversarial_loss=0.908085\n",
      "[INFO] Step 7_75: discriminator_loss=0.597650, adversarial_loss=0.847728\n",
      "[INFO] Step 7_100: discriminator_loss=0.626535, adversarial_loss=1.229970\n",
      "[INFO] Step 7_125: discriminator_loss=0.595914, adversarial_loss=0.933527\n",
      "[INFO] Step 7_150: discriminator_loss=0.614822, adversarial_loss=0.938603\n",
      "[INFO] Step 7_175: discriminator_loss=0.609415, adversarial_loss=0.684602\n",
      "[INFO] Step 7_200: discriminator_loss=0.556988, adversarial_loss=0.988404\n",
      "[INFO] Step 7_225: discriminator_loss=0.587860, adversarial_loss=0.861864\n",
      "[INFO] Step 7_250: discriminator_loss=0.609103, adversarial_loss=0.923527\n",
      "[INFO] Step 7_275: discriminator_loss=0.644050, adversarial_loss=0.607652\n",
      "[INFO] Step 7_300: discriminator_loss=0.576289, adversarial_loss=0.905689\n",
      "[INFO] Step 7_325: discriminator_loss=0.627868, adversarial_loss=1.032432\n",
      "[INFO] Step 7_350: discriminator_loss=0.570757, adversarial_loss=1.033651\n",
      "[INFO] Step 7_375: discriminator_loss=0.620154, adversarial_loss=1.388625\n",
      "[INFO] Step 7_400: discriminator_loss=0.628365, adversarial_loss=0.635426\n",
      "[INFO] Step 7_425: discriminator_loss=0.624075, adversarial_loss=0.763168\n",
      "[INFO] Step 7_450: discriminator_loss=0.625448, adversarial_loss=1.094390\n",
      "[INFO] Step 7_475: discriminator_loss=0.662690, adversarial_loss=0.514692\n",
      "[INFO] Step 7_500: discriminator_loss=0.600731, adversarial_loss=1.010516\n",
      "[INFO] Step 7_525: discriminator_loss=0.589924, adversarial_loss=0.819988\n",
      "[INFO] Step 7_545: discriminator_loss=0.647428, adversarial_loss=0.827221\n",
      "[INFO] starting epoch 8 of 50...\n",
      "[INFO] Step 8_0: discriminator_loss=0.598814, adversarial_loss=0.992508\n",
      "[INFO] Step 8_25: discriminator_loss=0.585694, adversarial_loss=0.659210\n",
      "[INFO] Step 8_50: discriminator_loss=0.614017, adversarial_loss=0.911896\n",
      "[INFO] Step 8_75: discriminator_loss=0.607603, adversarial_loss=1.084242\n",
      "[INFO] Step 8_100: discriminator_loss=0.582058, adversarial_loss=1.156492\n",
      "[INFO] Step 8_125: discriminator_loss=0.595652, adversarial_loss=0.841095\n",
      "[INFO] Step 8_150: discriminator_loss=0.606269, adversarial_loss=0.857615\n",
      "[INFO] Step 8_175: discriminator_loss=0.625482, adversarial_loss=0.783996\n",
      "[INFO] Step 8_200: discriminator_loss=0.594894, adversarial_loss=0.873502\n",
      "[INFO] Step 8_225: discriminator_loss=0.598597, adversarial_loss=0.871331\n",
      "[INFO] Step 8_250: discriminator_loss=0.613754, adversarial_loss=0.700890\n",
      "[INFO] Step 8_275: discriminator_loss=0.641947, adversarial_loss=0.638014\n",
      "[INFO] Step 8_300: discriminator_loss=0.587717, adversarial_loss=0.966630\n",
      "[INFO] Step 8_325: discriminator_loss=0.609367, adversarial_loss=0.783717\n",
      "[INFO] Step 8_350: discriminator_loss=0.573434, adversarial_loss=0.920759\n",
      "[INFO] Step 8_375: discriminator_loss=0.590477, adversarial_loss=1.240918\n",
      "[INFO] Step 8_400: discriminator_loss=0.611297, adversarial_loss=0.841240\n",
      "[INFO] Step 8_425: discriminator_loss=0.615617, adversarial_loss=0.745119\n",
      "[INFO] Step 8_450: discriminator_loss=0.616498, adversarial_loss=1.093185\n",
      "[INFO] Step 8_475: discriminator_loss=0.602674, adversarial_loss=0.665795\n",
      "[INFO] Step 8_500: discriminator_loss=0.583025, adversarial_loss=0.738539\n",
      "[INFO] Step 8_525: discriminator_loss=0.654425, adversarial_loss=0.622233\n",
      "[INFO] Step 8_545: discriminator_loss=0.615468, adversarial_loss=0.885305\n",
      "[INFO] starting epoch 9 of 50...\n",
      "[INFO] Step 9_0: discriminator_loss=0.602837, adversarial_loss=0.982452\n",
      "[INFO] Step 9_25: discriminator_loss=0.591220, adversarial_loss=0.833200\n",
      "[INFO] Step 9_50: discriminator_loss=0.598852, adversarial_loss=1.037861\n",
      "[INFO] Step 9_75: discriminator_loss=0.589968, adversarial_loss=0.960458\n",
      "[INFO] Step 9_100: discriminator_loss=0.606972, adversarial_loss=0.894282\n",
      "[INFO] Step 9_125: discriminator_loss=0.597163, adversarial_loss=1.013937\n",
      "[INFO] Step 9_150: discriminator_loss=0.625753, adversarial_loss=0.939152\n",
      "[INFO] Step 9_175: discriminator_loss=0.628012, adversarial_loss=0.952012\n",
      "[INFO] Step 9_200: discriminator_loss=0.607412, adversarial_loss=0.988642\n",
      "[INFO] Step 9_225: discriminator_loss=0.608411, adversarial_loss=0.784891\n",
      "[INFO] Step 9_250: discriminator_loss=0.592411, adversarial_loss=0.996196\n",
      "[INFO] Step 9_275: discriminator_loss=0.627600, adversarial_loss=0.819011\n",
      "[INFO] Step 9_300: discriminator_loss=0.611537, adversarial_loss=0.757411\n",
      "[INFO] Step 9_325: discriminator_loss=0.617673, adversarial_loss=0.836156\n",
      "[INFO] Step 9_350: discriminator_loss=0.608994, adversarial_loss=1.326920\n",
      "[INFO] Step 9_375: discriminator_loss=0.614616, adversarial_loss=1.304531\n",
      "[INFO] Step 9_400: discriminator_loss=0.608223, adversarial_loss=0.819530\n",
      "[INFO] Step 9_425: discriminator_loss=0.625267, adversarial_loss=0.834965\n",
      "[INFO] Step 9_450: discriminator_loss=0.602437, adversarial_loss=0.702788\n",
      "[INFO] Step 9_475: discriminator_loss=0.608709, adversarial_loss=0.725841\n",
      "[INFO] Step 9_500: discriminator_loss=0.610880, adversarial_loss=0.767196\n",
      "[INFO] Step 9_525: discriminator_loss=0.622308, adversarial_loss=0.632590\n",
      "[INFO] Step 9_545: discriminator_loss=0.587650, adversarial_loss=0.974284\n",
      "[INFO] starting epoch 10 of 50...\n",
      "[INFO] Step 10_0: discriminator_loss=0.622930, adversarial_loss=0.884039\n",
      "[INFO] Step 10_25: discriminator_loss=0.590918, adversarial_loss=0.756025\n",
      "[INFO] Step 10_50: discriminator_loss=0.628420, adversarial_loss=0.755835\n",
      "[INFO] Step 10_75: discriminator_loss=0.619760, adversarial_loss=1.181212\n",
      "[INFO] Step 10_100: discriminator_loss=0.639470, adversarial_loss=0.715687\n",
      "[INFO] Step 10_125: discriminator_loss=0.596769, adversarial_loss=1.114444\n",
      "[INFO] Step 10_150: discriminator_loss=0.614583, adversarial_loss=0.767872\n",
      "[INFO] Step 10_175: discriminator_loss=0.643009, adversarial_loss=0.669256\n",
      "[INFO] Step 10_200: discriminator_loss=0.606787, adversarial_loss=0.754882\n",
      "[INFO] Step 10_225: discriminator_loss=0.638748, adversarial_loss=0.742559\n",
      "[INFO] Step 10_250: discriminator_loss=0.598076, adversarial_loss=1.135529\n",
      "[INFO] Step 10_275: discriminator_loss=0.623461, adversarial_loss=0.734918\n",
      "[INFO] Step 10_300: discriminator_loss=0.584211, adversarial_loss=0.946168\n",
      "[INFO] Step 10_325: discriminator_loss=0.616287, adversarial_loss=0.981395\n",
      "[INFO] Step 10_350: discriminator_loss=0.559926, adversarial_loss=1.048143\n",
      "[INFO] Step 10_375: discriminator_loss=0.582683, adversarial_loss=1.222961\n",
      "[INFO] Step 10_400: discriminator_loss=0.629477, adversarial_loss=0.862489\n",
      "[INFO] Step 10_425: discriminator_loss=0.592641, adversarial_loss=0.875875\n",
      "[INFO] Step 10_450: discriminator_loss=0.592113, adversarial_loss=0.863107\n",
      "[INFO] Step 10_475: discriminator_loss=0.626752, adversarial_loss=0.743341\n",
      "[INFO] Step 10_500: discriminator_loss=0.620562, adversarial_loss=0.706676\n",
      "[INFO] Step 10_525: discriminator_loss=0.613101, adversarial_loss=0.769073\n",
      "[INFO] Step 10_545: discriminator_loss=0.589716, adversarial_loss=1.130247\n",
      "[INFO] starting epoch 11 of 50...\n",
      "[INFO] Step 11_0: discriminator_loss=0.601035, adversarial_loss=0.733835\n",
      "[INFO] Step 11_100: discriminator_loss=0.587481, adversarial_loss=0.982765\n",
      "[INFO] Step 11_200: discriminator_loss=0.596064, adversarial_loss=0.730002\n",
      "[INFO] Step 11_300: discriminator_loss=0.593280, adversarial_loss=0.755508\n",
      "[INFO] Step 11_400: discriminator_loss=0.662508, adversarial_loss=0.667546\n",
      "[INFO] Step 11_500: discriminator_loss=0.599935, adversarial_loss=1.178350\n",
      "[INFO] Step 11_545: discriminator_loss=0.590519, adversarial_loss=1.119871\n",
      "[INFO] starting epoch 12 of 50...\n",
      "[INFO] Step 12_0: discriminator_loss=0.573568, adversarial_loss=0.734430\n",
      "[INFO] Step 12_100: discriminator_loss=0.588222, adversarial_loss=0.947142\n",
      "[INFO] Step 12_200: discriminator_loss=0.563871, adversarial_loss=0.737177\n",
      "[INFO] Step 12_300: discriminator_loss=0.576389, adversarial_loss=0.880379\n",
      "[INFO] Step 12_400: discriminator_loss=0.643676, adversarial_loss=0.679080\n",
      "[INFO] Step 12_500: discriminator_loss=0.576405, adversarial_loss=0.821430\n",
      "[INFO] Step 12_545: discriminator_loss=0.569545, adversarial_loss=1.073767\n",
      "[INFO] starting epoch 13 of 50...\n",
      "[INFO] Step 13_0: discriminator_loss=0.576113, adversarial_loss=0.834991\n",
      "[INFO] Step 13_100: discriminator_loss=0.612665, adversarial_loss=1.108379\n",
      "[INFO] Step 13_200: discriminator_loss=0.575442, adversarial_loss=1.082033\n",
      "[INFO] Step 13_300: discriminator_loss=0.582129, adversarial_loss=0.924728\n",
      "[INFO] Step 13_400: discriminator_loss=0.613332, adversarial_loss=0.868648\n",
      "[INFO] Step 13_500: discriminator_loss=0.585825, adversarial_loss=0.756217\n",
      "[INFO] Step 13_545: discriminator_loss=0.601172, adversarial_loss=1.256275\n",
      "[INFO] starting epoch 14 of 50...\n",
      "[INFO] Step 14_0: discriminator_loss=0.618743, adversarial_loss=0.646527\n",
      "[INFO] Step 14_100: discriminator_loss=0.623670, adversarial_loss=0.822709\n",
      "[INFO] Step 14_200: discriminator_loss=0.571008, adversarial_loss=0.930850\n",
      "[INFO] Step 14_300: discriminator_loss=0.593129, adversarial_loss=0.803462\n",
      "[INFO] Step 14_400: discriminator_loss=0.612529, adversarial_loss=0.813713\n",
      "[INFO] Step 14_500: discriminator_loss=0.590524, adversarial_loss=0.827844\n",
      "[INFO] Step 14_545: discriminator_loss=0.575069, adversarial_loss=0.889457\n",
      "[INFO] starting epoch 15 of 50...\n",
      "[INFO] Step 15_0: discriminator_loss=0.564822, adversarial_loss=1.121589\n",
      "[INFO] Step 15_100: discriminator_loss=0.619794, adversarial_loss=1.095821\n",
      "[INFO] Step 15_200: discriminator_loss=0.578050, adversarial_loss=1.063892\n",
      "[INFO] Step 15_300: discriminator_loss=0.602267, adversarial_loss=1.013500\n",
      "[INFO] Step 15_400: discriminator_loss=0.629955, adversarial_loss=0.693391\n",
      "[INFO] Step 15_500: discriminator_loss=0.609708, adversarial_loss=0.999320\n",
      "[INFO] Step 15_545: discriminator_loss=0.598386, adversarial_loss=0.903769\n",
      "[INFO] starting epoch 16 of 50...\n",
      "[INFO] Step 16_0: discriminator_loss=0.621758, adversarial_loss=1.105046\n",
      "[INFO] Step 16_100: discriminator_loss=0.576673, adversarial_loss=0.683670\n",
      "[INFO] Step 16_200: discriminator_loss=0.529109, adversarial_loss=1.015142\n",
      "[INFO] Step 16_300: discriminator_loss=0.607329, adversarial_loss=0.779274\n",
      "[INFO] Step 16_400: discriminator_loss=0.612877, adversarial_loss=0.783149\n",
      "[INFO] Step 16_500: discriminator_loss=0.572001, adversarial_loss=0.880448\n",
      "[INFO] Step 16_545: discriminator_loss=0.569717, adversarial_loss=1.147974\n",
      "[INFO] starting epoch 17 of 50...\n",
      "[INFO] Step 17_0: discriminator_loss=0.589158, adversarial_loss=0.803106\n",
      "[INFO] Step 17_100: discriminator_loss=0.592485, adversarial_loss=0.816938\n",
      "[INFO] Step 17_200: discriminator_loss=0.601438, adversarial_loss=0.886890\n",
      "[INFO] Step 17_300: discriminator_loss=0.604282, adversarial_loss=0.675069\n",
      "[INFO] Step 17_400: discriminator_loss=0.610478, adversarial_loss=0.756670\n",
      "[INFO] Step 17_500: discriminator_loss=0.572836, adversarial_loss=0.885917\n",
      "[INFO] Step 17_545: discriminator_loss=0.582010, adversarial_loss=0.881073\n",
      "[INFO] starting epoch 18 of 50...\n",
      "[INFO] Step 18_0: discriminator_loss=0.570395, adversarial_loss=0.937901\n",
      "[INFO] Step 18_100: discriminator_loss=0.573435, adversarial_loss=1.117902\n",
      "[INFO] Step 18_200: discriminator_loss=0.556789, adversarial_loss=0.911968\n",
      "[INFO] Step 18_300: discriminator_loss=0.567773, adversarial_loss=0.997066\n",
      "[INFO] Step 18_400: discriminator_loss=0.642261, adversarial_loss=0.740259\n",
      "[INFO] Step 18_500: discriminator_loss=0.580625, adversarial_loss=1.069612\n",
      "[INFO] Step 18_545: discriminator_loss=0.585264, adversarial_loss=1.091885\n",
      "[INFO] starting epoch 19 of 50...\n",
      "[INFO] Step 19_0: discriminator_loss=0.570924, adversarial_loss=0.892184\n",
      "[INFO] Step 19_100: discriminator_loss=0.607202, adversarial_loss=0.722269\n",
      "[INFO] Step 19_200: discriminator_loss=0.558908, adversarial_loss=0.990607\n",
      "[INFO] Step 19_300: discriminator_loss=0.571956, adversarial_loss=0.957400\n",
      "[INFO] Step 19_400: discriminator_loss=0.590720, adversarial_loss=0.928252\n",
      "[INFO] Step 19_500: discriminator_loss=0.583253, adversarial_loss=0.974549\n",
      "[INFO] Step 19_545: discriminator_loss=0.567784, adversarial_loss=1.081734\n",
      "[INFO] starting epoch 20 of 50...\n",
      "[INFO] Step 20_0: discriminator_loss=0.563517, adversarial_loss=1.051070\n",
      "[INFO] Step 20_100: discriminator_loss=0.600422, adversarial_loss=1.401065\n",
      "[INFO] Step 20_200: discriminator_loss=0.531771, adversarial_loss=1.039354\n",
      "[INFO] Step 20_300: discriminator_loss=0.593252, adversarial_loss=0.832028\n",
      "[INFO] Step 20_400: discriminator_loss=0.654004, adversarial_loss=0.684088\n",
      "[INFO] Step 20_500: discriminator_loss=0.591404, adversarial_loss=0.880589\n",
      "[INFO] Step 20_545: discriminator_loss=0.627900, adversarial_loss=1.177804\n",
      "[INFO] starting epoch 21 of 50...\n",
      "[INFO] Step 21_0: discriminator_loss=0.593371, adversarial_loss=0.933800\n",
      "[INFO] Step 21_100: discriminator_loss=0.542008, adversarial_loss=1.106461\n",
      "[INFO] Step 21_200: discriminator_loss=0.570896, adversarial_loss=1.042121\n",
      "[INFO] Step 21_300: discriminator_loss=0.579151, adversarial_loss=0.887993\n",
      "[INFO] Step 21_400: discriminator_loss=0.622242, adversarial_loss=0.813040\n",
      "[INFO] Step 21_500: discriminator_loss=0.561567, adversarial_loss=0.962340\n",
      "[INFO] Step 21_545: discriminator_loss=0.586605, adversarial_loss=1.155931\n",
      "[INFO] starting epoch 22 of 50...\n",
      "[INFO] Step 22_0: discriminator_loss=0.567108, adversarial_loss=0.932663\n",
      "[INFO] Step 22_100: discriminator_loss=0.542043, adversarial_loss=1.117345\n",
      "[INFO] Step 22_200: discriminator_loss=0.550909, adversarial_loss=0.823821\n",
      "[INFO] Step 22_300: discriminator_loss=0.561313, adversarial_loss=0.970788\n",
      "[INFO] Step 22_400: discriminator_loss=0.615638, adversarial_loss=0.974394\n",
      "[INFO] Step 22_500: discriminator_loss=0.594557, adversarial_loss=0.906613\n",
      "[INFO] Step 22_545: discriminator_loss=0.638571, adversarial_loss=1.433812\n",
      "[INFO] starting epoch 23 of 50...\n",
      "[INFO] Step 23_0: discriminator_loss=0.602985, adversarial_loss=0.828187\n",
      "[INFO] Step 23_100: discriminator_loss=0.546258, adversarial_loss=1.152063\n",
      "[INFO] Step 23_200: discriminator_loss=0.566540, adversarial_loss=1.208512\n",
      "[INFO] Step 23_300: discriminator_loss=0.580109, adversarial_loss=0.964466\n",
      "[INFO] Step 23_400: discriminator_loss=0.608912, adversarial_loss=0.767630\n",
      "[INFO] Step 23_500: discriminator_loss=0.572443, adversarial_loss=1.063617\n",
      "[INFO] Step 23_545: discriminator_loss=0.607464, adversarial_loss=1.315707\n",
      "[INFO] starting epoch 24 of 50...\n",
      "[INFO] Step 24_0: discriminator_loss=0.564022, adversarial_loss=0.828389\n",
      "[INFO] Step 24_100: discriminator_loss=0.592017, adversarial_loss=1.367985\n",
      "[INFO] Step 24_200: discriminator_loss=0.550690, adversarial_loss=1.077905\n",
      "[INFO] Step 24_300: discriminator_loss=0.614131, adversarial_loss=0.732629\n",
      "[INFO] Step 24_400: discriminator_loss=0.630839, adversarial_loss=0.631853\n",
      "[INFO] Step 24_500: discriminator_loss=0.581243, adversarial_loss=0.732449\n",
      "[INFO] Step 24_545: discriminator_loss=0.568102, adversarial_loss=1.312034\n",
      "[INFO] starting epoch 25 of 50...\n",
      "[INFO] Step 25_0: discriminator_loss=0.555491, adversarial_loss=1.147374\n",
      "[INFO] Step 25_100: discriminator_loss=0.556784, adversarial_loss=1.258411\n",
      "[INFO] Step 25_200: discriminator_loss=0.534151, adversarial_loss=1.019291\n",
      "[INFO] Step 25_300: discriminator_loss=0.587208, adversarial_loss=1.129077\n",
      "[INFO] Step 25_400: discriminator_loss=0.560563, adversarial_loss=0.747995\n",
      "[INFO] Step 25_500: discriminator_loss=0.594093, adversarial_loss=1.135865\n",
      "[INFO] Step 25_545: discriminator_loss=0.588545, adversarial_loss=1.241062\n",
      "[INFO] starting epoch 26 of 50...\n",
      "[INFO] Step 26_0: discriminator_loss=0.559676, adversarial_loss=1.243089\n",
      "[INFO] Step 26_100: discriminator_loss=0.633925, adversarial_loss=1.692853\n",
      "[INFO] Step 26_200: discriminator_loss=0.556936, adversarial_loss=1.057788\n",
      "[INFO] Step 26_300: discriminator_loss=0.577883, adversarial_loss=1.337591\n",
      "[INFO] Step 26_400: discriminator_loss=0.601281, adversarial_loss=1.215718\n",
      "[INFO] Step 26_500: discriminator_loss=0.557686, adversarial_loss=1.081691\n",
      "[INFO] Step 26_545: discriminator_loss=0.579898, adversarial_loss=1.146976\n",
      "[INFO] starting epoch 27 of 50...\n",
      "[INFO] Step 27_0: discriminator_loss=0.545658, adversarial_loss=1.158486\n",
      "[INFO] Step 27_100: discriminator_loss=0.604302, adversarial_loss=1.612680\n",
      "[INFO] Step 27_200: discriminator_loss=0.574668, adversarial_loss=1.457421\n",
      "[INFO] Step 27_300: discriminator_loss=0.535049, adversarial_loss=1.030086\n",
      "[INFO] Step 27_400: discriminator_loss=0.566224, adversarial_loss=0.789258\n",
      "[INFO] Step 27_500: discriminator_loss=0.597288, adversarial_loss=0.990863\n",
      "[INFO] Step 27_545: discriminator_loss=0.542932, adversarial_loss=1.100381\n",
      "[INFO] starting epoch 28 of 50...\n",
      "[INFO] Step 28_0: discriminator_loss=0.538718, adversarial_loss=1.138419\n",
      "[INFO] Step 28_100: discriminator_loss=0.541622, adversarial_loss=1.282903\n",
      "[INFO] Step 28_200: discriminator_loss=0.518444, adversarial_loss=1.523603\n",
      "[INFO] Step 28_300: discriminator_loss=0.571732, adversarial_loss=1.141959\n",
      "[INFO] Step 28_400: discriminator_loss=0.573743, adversarial_loss=0.726734\n",
      "[INFO] Step 28_500: discriminator_loss=0.551641, adversarial_loss=0.889654\n",
      "[INFO] Step 28_545: discriminator_loss=0.518611, adversarial_loss=1.040586\n",
      "[INFO] starting epoch 29 of 50...\n",
      "[INFO] Step 29_0: discriminator_loss=0.512542, adversarial_loss=1.214007\n",
      "[INFO] Step 29_100: discriminator_loss=0.533335, adversarial_loss=1.242401\n",
      "[INFO] Step 29_200: discriminator_loss=0.545098, adversarial_loss=1.259016\n",
      "[INFO] Step 29_300: discriminator_loss=0.580947, adversarial_loss=0.642085\n",
      "[INFO] Step 29_400: discriminator_loss=0.603036, adversarial_loss=0.883246\n",
      "[INFO] Step 29_500: discriminator_loss=0.561044, adversarial_loss=0.812444\n",
      "[INFO] Step 29_545: discriminator_loss=0.553417, adversarial_loss=1.171203\n",
      "[INFO] starting epoch 30 of 50...\n",
      "[INFO] Step 30_0: discriminator_loss=0.504890, adversarial_loss=0.978534\n",
      "[INFO] Step 30_100: discriminator_loss=0.549067, adversarial_loss=1.134568\n",
      "[INFO] Step 30_200: discriminator_loss=0.523936, adversarial_loss=0.973500\n",
      "[INFO] Step 30_300: discriminator_loss=0.537315, adversarial_loss=1.335705\n",
      "[INFO] Step 30_400: discriminator_loss=0.573039, adversarial_loss=0.822183\n",
      "[INFO] Step 30_500: discriminator_loss=0.562537, adversarial_loss=0.870828\n",
      "[INFO] Step 30_545: discriminator_loss=0.534695, adversarial_loss=1.156670\n",
      "[INFO] starting epoch 31 of 50...\n",
      "[INFO] Step 31_0: discriminator_loss=0.520121, adversarial_loss=1.074134\n",
      "[INFO] Step 31_100: discriminator_loss=0.545617, adversarial_loss=1.345773\n",
      "[INFO] Step 31_200: discriminator_loss=0.508462, adversarial_loss=1.077935\n",
      "[INFO] Step 31_300: discriminator_loss=0.510177, adversarial_loss=0.900608\n",
      "[INFO] Step 31_400: discriminator_loss=0.584841, adversarial_loss=0.765022\n",
      "[INFO] Step 31_500: discriminator_loss=0.563056, adversarial_loss=0.948852\n",
      "[INFO] Step 31_545: discriminator_loss=0.540101, adversarial_loss=1.061156\n",
      "[INFO] starting epoch 32 of 50...\n",
      "[INFO] Step 32_0: discriminator_loss=0.508450, adversarial_loss=1.224481\n",
      "[INFO] Step 32_100: discriminator_loss=0.524911, adversarial_loss=1.058049\n",
      "[INFO] Step 32_200: discriminator_loss=0.542041, adversarial_loss=1.208265\n",
      "[INFO] Step 32_300: discriminator_loss=0.528867, adversarial_loss=0.893640\n",
      "[INFO] Step 32_400: discriminator_loss=0.529252, adversarial_loss=0.950751\n",
      "[INFO] Step 32_500: discriminator_loss=0.521877, adversarial_loss=1.083361\n",
      "[INFO] Step 32_545: discriminator_loss=0.580615, adversarial_loss=1.402348\n",
      "[INFO] starting epoch 33 of 50...\n",
      "[INFO] Step 33_0: discriminator_loss=0.553981, adversarial_loss=0.965732\n",
      "[INFO] Step 33_100: discriminator_loss=0.544075, adversarial_loss=1.477312\n",
      "[INFO] Step 33_200: discriminator_loss=0.517846, adversarial_loss=1.143599\n",
      "[INFO] Step 33_300: discriminator_loss=0.525925, adversarial_loss=1.149462\n",
      "[INFO] Step 33_400: discriminator_loss=0.573682, adversarial_loss=1.043959\n",
      "[INFO] Step 33_500: discriminator_loss=0.570648, adversarial_loss=0.800616\n",
      "[INFO] Step 33_545: discriminator_loss=0.533012, adversarial_loss=0.879645\n",
      "[INFO] starting epoch 34 of 50...\n",
      "[INFO] Step 34_0: discriminator_loss=0.541483, adversarial_loss=1.392608\n",
      "[INFO] Step 34_100: discriminator_loss=0.565764, adversarial_loss=1.423243\n",
      "[INFO] Step 34_200: discriminator_loss=0.507448, adversarial_loss=0.993631\n",
      "[INFO] Step 34_300: discriminator_loss=0.539287, adversarial_loss=1.187557\n",
      "[INFO] Step 34_400: discriminator_loss=0.558149, adversarial_loss=1.181379\n",
      "[INFO] Step 34_500: discriminator_loss=0.520309, adversarial_loss=0.885874\n",
      "[INFO] Step 34_545: discriminator_loss=0.512838, adversarial_loss=1.075217\n",
      "[INFO] starting epoch 35 of 50...\n",
      "[INFO] Step 35_0: discriminator_loss=0.517206, adversarial_loss=1.216585\n",
      "[INFO] Step 35_100: discriminator_loss=0.522796, adversarial_loss=1.333766\n",
      "[INFO] Step 35_200: discriminator_loss=0.482557, adversarial_loss=1.239714\n",
      "[INFO] Step 35_300: discriminator_loss=0.513041, adversarial_loss=0.972604\n",
      "[INFO] Step 35_400: discriminator_loss=0.566130, adversarial_loss=0.991970\n",
      "[INFO] Step 35_500: discriminator_loss=0.515027, adversarial_loss=1.081546\n",
      "[INFO] Step 35_545: discriminator_loss=0.514949, adversarial_loss=1.203279\n",
      "[INFO] starting epoch 36 of 50...\n",
      "[INFO] Step 36_0: discriminator_loss=0.505763, adversarial_loss=1.097790\n",
      "[INFO] Step 36_100: discriminator_loss=0.522743, adversarial_loss=1.104475\n",
      "[INFO] Step 36_200: discriminator_loss=0.520760, adversarial_loss=1.219826\n",
      "[INFO] Step 36_300: discriminator_loss=0.571452, adversarial_loss=1.535310\n",
      "[INFO] Step 36_400: discriminator_loss=0.551842, adversarial_loss=1.142850\n",
      "[INFO] Step 36_500: discriminator_loss=0.549884, adversarial_loss=1.148095\n",
      "[INFO] Step 36_545: discriminator_loss=0.516681, adversarial_loss=0.869870\n",
      "[INFO] starting epoch 37 of 50...\n",
      "[INFO] Step 37_0: discriminator_loss=0.560200, adversarial_loss=1.626485\n",
      "[INFO] Step 37_100: discriminator_loss=0.543030, adversarial_loss=1.392488\n",
      "[INFO] Step 37_200: discriminator_loss=0.546236, adversarial_loss=1.342273\n",
      "[INFO] Step 37_300: discriminator_loss=0.538715, adversarial_loss=1.255360\n",
      "[INFO] Step 37_400: discriminator_loss=0.591697, adversarial_loss=1.565487\n",
      "[INFO] Step 37_500: discriminator_loss=0.511426, adversarial_loss=0.969015\n",
      "[INFO] Step 37_545: discriminator_loss=0.515047, adversarial_loss=1.212382\n",
      "[INFO] starting epoch 38 of 50...\n",
      "[INFO] Step 38_0: discriminator_loss=0.516335, adversarial_loss=1.149612\n",
      "[INFO] Step 38_100: discriminator_loss=0.503713, adversarial_loss=1.397339\n",
      "[INFO] Step 38_200: discriminator_loss=0.529684, adversarial_loss=1.405757\n",
      "[INFO] Step 38_300: discriminator_loss=0.524881, adversarial_loss=1.541362\n",
      "[INFO] Step 38_400: discriminator_loss=0.541463, adversarial_loss=0.842099\n",
      "[INFO] Step 38_500: discriminator_loss=0.519193, adversarial_loss=1.003052\n",
      "[INFO] Step 38_545: discriminator_loss=0.511405, adversarial_loss=1.158699\n",
      "[INFO] starting epoch 39 of 50...\n",
      "[INFO] Step 39_0: discriminator_loss=0.506710, adversarial_loss=1.258478\n",
      "[INFO] Step 39_100: discriminator_loss=0.529568, adversarial_loss=1.381513\n",
      "[INFO] Step 39_200: discriminator_loss=0.503416, adversarial_loss=1.146103\n",
      "[INFO] Step 39_300: discriminator_loss=0.528266, adversarial_loss=0.962382\n",
      "[INFO] Step 39_400: discriminator_loss=0.575513, adversarial_loss=1.016985\n",
      "[INFO] Step 39_500: discriminator_loss=0.560625, adversarial_loss=0.832007\n",
      "[INFO] Step 39_545: discriminator_loss=0.526130, adversarial_loss=0.979423\n",
      "[INFO] starting epoch 40 of 50...\n",
      "[INFO] Step 40_0: discriminator_loss=0.548978, adversarial_loss=1.691451\n",
      "[INFO] Step 40_100: discriminator_loss=0.579686, adversarial_loss=1.704335\n",
      "[INFO] Step 40_200: discriminator_loss=0.483382, adversarial_loss=1.253694\n",
      "[INFO] Step 40_300: discriminator_loss=0.522627, adversarial_loss=1.294175\n",
      "[INFO] Step 40_400: discriminator_loss=0.598979, adversarial_loss=0.797167\n",
      "[INFO] Step 40_500: discriminator_loss=0.509715, adversarial_loss=0.977782\n",
      "[INFO] Step 40_545: discriminator_loss=0.526534, adversarial_loss=1.115530\n",
      "[INFO] starting epoch 41 of 50...\n",
      "[INFO] Step 41_0: discriminator_loss=0.502051, adversarial_loss=1.212334\n",
      "[INFO] Step 41_100: discriminator_loss=0.565298, adversarial_loss=1.709486\n",
      "[INFO] Step 41_200: discriminator_loss=0.472663, adversarial_loss=1.068207\n",
      "[INFO] Step 41_300: discriminator_loss=0.500277, adversarial_loss=0.986790\n",
      "[INFO] Step 41_400: discriminator_loss=0.571538, adversarial_loss=0.868515\n",
      "[INFO] Step 41_500: discriminator_loss=0.547422, adversarial_loss=1.063407\n",
      "[INFO] Step 41_545: discriminator_loss=0.562346, adversarial_loss=1.474084\n",
      "[INFO] starting epoch 42 of 50...\n",
      "[INFO] Step 42_0: discriminator_loss=0.545189, adversarial_loss=0.953541\n",
      "[INFO] Step 42_100: discriminator_loss=0.574224, adversarial_loss=1.854995\n",
      "[INFO] Step 42_200: discriminator_loss=0.472068, adversarial_loss=1.136286\n",
      "[INFO] Step 42_300: discriminator_loss=0.482139, adversarial_loss=1.074186\n",
      "[INFO] Step 42_400: discriminator_loss=0.560197, adversarial_loss=1.061137\n",
      "[INFO] Step 42_500: discriminator_loss=0.525127, adversarial_loss=1.019438\n",
      "[INFO] Step 42_545: discriminator_loss=0.568924, adversarial_loss=1.055596\n",
      "[INFO] starting epoch 43 of 50...\n",
      "[INFO] Step 43_0: discriminator_loss=0.518553, adversarial_loss=1.585974\n",
      "[INFO] Step 43_100: discriminator_loss=0.536043, adversarial_loss=1.669223\n",
      "[INFO] Step 43_200: discriminator_loss=0.461669, adversarial_loss=1.058253\n",
      "[INFO] Step 43_300: discriminator_loss=0.518956, adversarial_loss=1.180920\n",
      "[INFO] Step 43_400: discriminator_loss=0.572338, adversarial_loss=0.935410\n",
      "[INFO] Step 43_500: discriminator_loss=0.525339, adversarial_loss=1.057400\n",
      "[INFO] Step 43_545: discriminator_loss=0.515971, adversarial_loss=1.180537\n",
      "[INFO] starting epoch 44 of 50...\n",
      "[INFO] Step 44_0: discriminator_loss=0.515260, adversarial_loss=1.423890\n",
      "[INFO] Step 44_100: discriminator_loss=0.543694, adversarial_loss=1.686738\n",
      "[INFO] Step 44_200: discriminator_loss=0.520879, adversarial_loss=1.513422\n",
      "[INFO] Step 44_300: discriminator_loss=0.476875, adversarial_loss=1.083433\n",
      "[INFO] Step 44_400: discriminator_loss=0.594154, adversarial_loss=0.640761\n",
      "[INFO] Step 44_500: discriminator_loss=0.515606, adversarial_loss=1.078903\n",
      "[INFO] Step 44_545: discriminator_loss=0.480105, adversarial_loss=1.062079\n",
      "[INFO] starting epoch 45 of 50...\n",
      "[INFO] Step 45_0: discriminator_loss=0.518900, adversarial_loss=1.265484\n",
      "[INFO] Step 45_100: discriminator_loss=0.543271, adversarial_loss=1.857741\n",
      "[INFO] Step 45_200: discriminator_loss=0.480874, adversarial_loss=1.040953\n",
      "[INFO] Step 45_300: discriminator_loss=0.515338, adversarial_loss=1.229362\n",
      "[INFO] Step 45_400: discriminator_loss=0.558885, adversarial_loss=0.965345\n",
      "[INFO] Step 45_500: discriminator_loss=0.510379, adversarial_loss=0.997145\n",
      "[INFO] Step 45_545: discriminator_loss=0.496916, adversarial_loss=1.084523\n",
      "[INFO] starting epoch 46 of 50...\n",
      "[INFO] Step 46_0: discriminator_loss=0.538612, adversarial_loss=1.504829\n",
      "[INFO] Step 46_100: discriminator_loss=0.531924, adversarial_loss=1.579172\n",
      "[INFO] Step 46_200: discriminator_loss=0.491722, adversarial_loss=1.255608\n",
      "[INFO] Step 46_300: discriminator_loss=0.485979, adversarial_loss=1.179751\n",
      "[INFO] Step 46_400: discriminator_loss=0.560876, adversarial_loss=1.202546\n",
      "[INFO] Step 46_500: discriminator_loss=0.511356, adversarial_loss=1.007548\n",
      "[INFO] Step 46_545: discriminator_loss=0.490451, adversarial_loss=1.262758\n",
      "[INFO] starting epoch 47 of 50...\n",
      "[INFO] Step 47_0: discriminator_loss=0.503233, adversarial_loss=1.319973\n",
      "[INFO] Step 47_100: discriminator_loss=0.498313, adversarial_loss=1.676961\n",
      "[INFO] Step 47_200: discriminator_loss=0.476723, adversarial_loss=1.105217\n",
      "[INFO] Step 47_300: discriminator_loss=0.492312, adversarial_loss=1.133063\n",
      "[INFO] Step 47_400: discriminator_loss=0.606177, adversarial_loss=0.764198\n",
      "[INFO] Step 47_500: discriminator_loss=0.515554, adversarial_loss=1.254888\n",
      "[INFO] Step 47_545: discriminator_loss=0.478657, adversarial_loss=1.353361\n",
      "[INFO] starting epoch 48 of 50...\n",
      "[INFO] Step 48_0: discriminator_loss=0.477387, adversarial_loss=1.254263\n",
      "[INFO] Step 48_100: discriminator_loss=0.481588, adversarial_loss=1.482164\n",
      "[INFO] Step 48_200: discriminator_loss=0.504264, adversarial_loss=1.313342\n",
      "[INFO] Step 48_300: discriminator_loss=0.490061, adversarial_loss=1.112494\n",
      "[INFO] Step 48_400: discriminator_loss=0.544116, adversarial_loss=1.002662\n",
      "[INFO] Step 48_500: discriminator_loss=0.547634, adversarial_loss=1.167866\n",
      "[INFO] Step 48_545: discriminator_loss=0.542486, adversarial_loss=1.431219\n",
      "[INFO] starting epoch 49 of 50...\n",
      "[INFO] Step 49_0: discriminator_loss=0.548006, adversarial_loss=1.355702\n",
      "[INFO] Step 49_100: discriminator_loss=0.604093, adversarial_loss=1.995359\n",
      "[INFO] Step 49_200: discriminator_loss=0.455538, adversarial_loss=1.239004\n",
      "[INFO] Step 49_300: discriminator_loss=0.471871, adversarial_loss=1.168806\n",
      "[INFO] Step 49_400: discriminator_loss=0.558603, adversarial_loss=1.162760\n",
      "[INFO] Step 49_500: discriminator_loss=0.509115, adversarial_loss=0.934937\n",
      "[INFO] Step 49_545: discriminator_loss=0.504614, adversarial_loss=1.068227\n",
      "[INFO] starting epoch 50 of 50...\n",
      "[INFO] Step 50_0: discriminator_loss=0.490256, adversarial_loss=1.496553\n",
      "[INFO] Step 50_100: discriminator_loss=0.462340, adversarial_loss=1.732623\n",
      "[INFO] Step 50_200: discriminator_loss=0.490720, adversarial_loss=1.190289\n",
      "[INFO] Step 50_300: discriminator_loss=0.483237, adversarial_loss=1.093657\n",
      "[INFO] Step 50_400: discriminator_loss=0.587437, adversarial_loss=1.405836\n",
      "[INFO] Step 50_500: discriminator_loss=0.531455, adversarial_loss=1.078262\n",
      "[INFO] Step 50_545: discriminator_loss=0.473249, adversarial_loss=1.213978\n"
     ]
    }
   ],
   "source": [
    "# randomly generate some benchmark noise so we can consistently\n",
    "# visualize how the generative modeling is learning\n",
    "print(\"[INFO] starting training...\")\n",
    "benchmarkNoise = np.random.uniform(-1, 1, size=(256, 100))\n",
    "#benchmarkNoise is generated from a uniform distribution in the range [-1, 1], the same range as our tanh activation function\n",
    "#size(256,100) indicates that we'll be generating 256 synthetic images, where each input starts as a 100-d vector\n",
    "\n",
    "# loop over the epochs\n",
    "for epoch in range(0, NUM_EPOCHS):\n",
    "\t# show epoch information and compute the number of batches per\n",
    "\t# epoch\n",
    "\tprint(\"[INFO] starting epoch {} of {}...\".format(epoch + 1,\n",
    "\t\tNUM_EPOCHS))\n",
    "\tbatchesPerEpoch = int(trainImages.shape[0] / BATCH_SIZE)\n",
    "\n",
    "\t# loop over the batches\n",
    "\tfor i in range(0, batchesPerEpoch):\n",
    "\t\t# initialize an (empty) output path\n",
    "\t\tp = None\n",
    "\n",
    "\t\t# select the next batch of images, then randomly generate\n",
    "\t\t# noise for the generator to predict on\n",
    "\t\timageBatch = trainImages[i * BATCH_SIZE:(i + 1) * BATCH_SIZE]\n",
    "\t\tnoise = np.random.uniform(-1, 1, size=(BATCH_SIZE, 100))\n",
    "\n",
    "\t\t# generate images using the noise + generator model\n",
    "\t\tgenImages = gen.predict(noise, verbose=0)\n",
    "\n",
    "\t\t# concatenate the *actual* images and the *generated* images,\n",
    "\t\t# construct class labels for the discriminator, and shuffle\n",
    "\t\t# the data\n",
    "\t\tX = np.concatenate((imageBatch, genImages))\n",
    "\t\ty = ([1] * BATCH_SIZE) + ([0] * BATCH_SIZE)\n",
    "\t\ty = np.reshape(y, (-1,))\n",
    "\t\t(X, y) = shuffle(X, y)\n",
    "\n",
    "\t\t# train the discriminator on the data\n",
    "\t\tdiscLoss = disc.train_on_batch(X, y)\n",
    "  \n",
    "\t\t# let's now train our generator via the adversarial model by\n",
    "\t\t# (1) generating random noise and (2) training the generator\n",
    "\t\t# with the discriminator weights frozen\n",
    "\t\tnoise = np.random.uniform(-1, 1, (BATCH_SIZE, 100))\n",
    "\t\tfakeLabels = [1] * BATCH_SIZE\n",
    "\t\tfakeLabels = np.reshape(fakeLabels, (-1,))\n",
    "\t\tganLoss = gan.train_on_batch(noise, fakeLabels)\n",
    "\n",
    "\t\t# check to see if this is the end of an epoch, and if so,\n",
    "\t\t# initialize the output path\n",
    "\t\tif i == batchesPerEpoch - 1:\n",
    "\t\t\tp = [args[\"output\"], \"epoch_{}_output.png\".format(\n",
    "\t\t\t\tstr(epoch + 1).zfill(4))]\n",
    "\n",
    "\t\t# otherwise, check to see if we should visualize the current\n",
    "\t\t# batch for the epoch\n",
    "\t\telse:\n",
    "\t\t\t# create more visualizations early in the training\n",
    "\t\t\t# process\n",
    "\t\t\tif epoch < 10 and i % 25 == 0:\n",
    "\t\t\t\tp = [args[\"output\"], \"epoch_{}_step_{}.png\".format(\n",
    "\t\t\t\t\tstr(epoch + 1).zfill(4), str(i).zfill(5))]\n",
    "\n",
    "\t\t\t# visualizations later in the training process are less\n",
    "\t\t\t# interesting\n",
    "\t\t\telif epoch >= 10 and i % 100 == 0:\n",
    "\t\t\t\tp = [args[\"output\"], \"epoch_{}_step_{}.png\".format(\n",
    "\t\t\t\t\tstr(epoch + 1).zfill(4), str(i).zfill(5))]\n",
    "\n",
    "\t\t# check to see if we should visualize the output of the\n",
    "\t\t# generator model on our benchmark data\n",
    "\t\tif p is not None:\n",
    "\t\t\t# show loss information\n",
    "\t\t\tprint(\"[INFO] Step {}_{}: discriminator_loss={:.6f}, \"\n",
    "\t\t\t\t\"adversarial_loss={:.6f}\".format(epoch + 1, i,\n",
    "\t\t\t\t\tdiscLoss, ganLoss))\n",
    "\n",
    "\t\t\t# make predictions on the benchmark noise, scale it back\n",
    "\t\t\t# to the range [0, 255], and generate the montage\n",
    "\t\t\timages = gen.predict(benchmarkNoise)\n",
    "\t\t\timages = ((images * 127.5) + 127.5).astype(\"uint8\")\n",
    "\t\t\timages = np.repeat(images, 3, axis=-1)\n",
    "\t\t\tvis = build_montages(images, (28, 28), (16, 16))[0]\n",
    "\n",
    "\t\t\t# write the visualization to disk\n",
    "\t\t\tp = os.path.sep.join(p)\n",
    "\t\t\tcv2.imwrite(p, vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WuptzEe3iMUX"
   },
   "source": [
    "### Visualizing our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "id": "e4BrHJaGiLdC",
    "outputId": "7a3c2ac0-842a-4f8f-a7a5-bd3389de32e0"
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-2ec1e28958fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# load the result image and display it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresultPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mplt_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"result\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-ac32161eaba1>\u001b[0m in \u001b[0;36mplt_imshow\u001b[0;34m(title, image)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplt_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;31m# convert the image frame BGR to RGB color space and display it\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maspect\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: OpenCV(4.1.2) /io/opencv/modules/imgproc/src/color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cvtColor'\n"
     ]
    }
   ],
   "source": [
    "# define path to our final epoch results\n",
    "filename = \"epoch_{}_output.png\".format(str(args[\"epochs\"]).zfill(4))\n",
    "resultPath = os.path.join(args[\"output\"], filename)\n",
    "\n",
    "# load the result image and display it\n",
    "image = cv2.imread(resultPath)\n",
    "plt_imshow(\"result\", image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4ogkNauArL6u"
   },
   "source": [
    "For a detailed walkthrough of the concepts and code, be sure to refer to the full tutorial, [*GANs with Keras and TensorFlow*](https://www.pyimagesearch.com/2020/11/16/gans-with-keras-and-tensorflow/) published on 2020-11-16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUhIbZsz5JoR"
   },
   "source": [
    "# Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QA1QOqab4vWv"
   },
   "source": [
    "# Citation\n",
    "\n",
    "https://arxiv.org/abs/1406.2661 <br/>\n",
    "https://towardsdatascience.com/understanding-generative-adversarial-networks-gans-cd6e4651a29 <br/>\n",
    "https://www.geeksforgeeks.org/generative-adversarial-network-gan/ <br/>\n",
    "https://naokishibuya.medium.com/up-sampling-with-transposed-convolution-9ae4f2df52d0<br/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2KiSc7FH4dXE"
   },
   "source": [
    "# License\n",
    "\n",
    "Copyright 2020 Rohan Subhash Yewale\n",
    "\n",
    "Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I6IY7Jl74uLN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Yewale_Rohan_Mini_Project_3.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
